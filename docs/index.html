<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <title>Discovering Music Relations with Sequential Attention</title>
	<style>
		.figure {display: table; margin-right: auto; margin-left: auto;}
		h4 {padding-top: 20px; padding-bottom: 10px}
		h2 {padding-top: 50px; padding-bottom: 20px}
		.midi-container {max-width: 800px; height: 300px; margin-right: auto; margin-left: auto;}
	</style>
  </head>
  <body>
	<div class="container">
    <h2 class="text-center">Discovering Music Relations with Sequential Attention</h2>
	
	<div class="text-center">
	Junyan Jiang<span>&#185;</span>, Gus G. Xia<span>&#185;</span>, Taylor Berg-Kirkpatrick<span>&#178;</span><br />
	<span>&#185;</span>MusicX Lab, NYU Shanghai <br />
	<span>&#178;</span>University of California San Diego
	
	</div>
	<br/>
	<div class="text-center">
	<a href="https://github.com/music-x-lab/SeqAttn">Code repository</a> &bull; <a href="https://drive.google.com/file/d/1qWAnX2QoNvArdk6tktlRVtWismoDGd2b/view">Paper</a> &bull; <a href="https://www.youtube.com/watch?v=-AhiKghIyOw&list=PL44xXQ2KNZ0Inxy6ZIol47RkmPWWRCTBQ&index=5">Video</a>
	</div>
	
	<div class="mt-4 container">
	
	<p>Notice: If you have trouble with the interactive MIDI players on the page, consider using Chrome, Edge or Firefox. You can drag the piano roll using mouse or touch devices to change the playing position.</p>
	
    <div class="embed-responsive midi-container">
         <iframe class="embed-responsive-item" src="player.html#midi/3_given_0bars_generate_64bars.mid"> </iframe>
    </div>
	<figure class="figure text-center">
    <figcaption class="figure-caption">A piece generated by the proposed model. The melody line (purple) is generated given the chords (red). More generated MIDI samples are available <a href="https://github.com/music-x-lab/SeqAttn/tree/master/midi">here</a></figcaption>
	</figure>
	<h4> 1. Introduction </h4>
	
	<p>Music is a type of sequential data with various kinds of long-term relations, including repetition, retrograde, sequences, call-and-responses, and many more. It is crucial to model such (potentially long-term) relations in both music analysis and generation tasks. </p>
	
	<p>Currently, attentive models like transformers are a popular method to capture long-term relations in a sequence. The main mechanism in these models is the <b>element-wise attention mechanism</b>. It is powerful at capturing element-wise similarity, but lacks inductive bias to directly compare sequences against sequences. It also requires a multi-layer setting to capture sequence-level similarity. </p>
	
	<p>In this paper, we present the <b>sequential attention module</b> that directly models sequence-level relations in music. In this module, the type of keys and queries are no longer tokens, but sequences.</p>
	
	<table class="table">
	  <thead>
		<tr>
		  <th scope="col">Attention module</th>
		  <th scope="col">Element-wise attention</th>
		  <th scope="col">Sequential attention (this paper)</th>
		</tr>
	  </thead>
	  <tbody>
		<tr>
		  <th scope="row">Type of keys</th>
		  <td>Token</td>
		  <td>Sequence</td>
		</tr>
		<tr>
		  <th scope="row">Type of querys</th>
		  <td>Token</td>
		  <td>Sequence</td>
		</tr>
		<tr>
		  <th scope="row">Weighting method</th>
		  <td>Dot(key, query)</td>
		  <td>FFN(LSTM(Concat(key, query)))</td>
		</tr>
	  </tbody>
	</table>
</p>
	
	<h4> 2. Module Architecture </h4>
	<p>To perform sequence-wise similarity calculation, we first stack the key and query sequence together, and then feed them into a uni-directional Long Short-Term Memory (LSTM) layer. The output of the LSTM and the corresponding key token will be used to determine the matching score of the two sequences, and a predicted token for the next token of the query string. </p>
	<figure class="figure text-center">
	  <img src="img/Architecture.png" class="figure-img img-fluid" style="width: 100%; max-width: 600px;">
	  <figcaption class="figure-caption">Fig. 1. The module architecture.</figcaption>
	</figure>
	
	<p>Fig. 1 provides an illustrative scenario where we have two sequences of notes (Fig. 1). The first sequence is <code>C4 D4 E4 C4 G4</code>, and the second is <code>A3 B3 C4 A3 ?</code> where the question mark denotes an unknown token we want to predict. Notice that these two strings are likely to form a <b>tonal sequence</b> relation, and we can use this information to predict the unknown token is likely to be <code>E4</code>. If the module is well-trained, it will discover similar relations and use them to improve the prediction accuracy.</p>
	
	<figure class="figure text-center">
	  <img src="img/Overview.png" class="figure-img img-fluid" style="width: 100%; max-width: 500px;">
	  <figcaption class="figure-caption">Fig. 2. Self-attentive layer using sequential attention modules.</figcaption>
	</figure>
	<p>We can use this module in an attentive language model (Fig. 2). To predict the next token of a partial sequence, we can regard its suffix as the query string, and its substrings as the key strings. Notice that some key strings are not well-matched with the query string, providing useless information for prediction. The normalized matching score is used as the attention weights since a higher score indicates a more important relation between the key and query sequences. We aggregate the prediction by a weighted average layer to produce the final prediction for the next token.</p>

	<figure class="figure text-center">
	  <img src="img/Conditional.png" class="figure-img img-fluid" style="width: 100%; max-width: 500px;">
	  <figcaption class="figure-caption">Fig. 3. A conditional version of the sequential attention module.</figcaption>
	</figure>
	<p>For the task of conditional sequence generation (e.g., melody generation given chord sequences), we propose the conditional version of the sequential attention module (Fig. 3). In this module, the relations of the condition sequences are also considered. Since future conditions are also revealed, the module contains a backward LSTM to capture the relations of future conditions.</p>
	
	<h4> 3. Results </h4>
	<figure class="figure text-center">
	  <img src="img/Results.png" class="figure-img img-fluid" style="width: 100%; max-width: 400px;">
	  <figcaption class="figure-caption">Fig. 4. The comparative results for the accuracy and the perplexity of the next token prediction task on test sets.</figcaption>
	</figure>
	<p>We showed by experiments that the model outperforms a 3-layer transformer model with relative positional encoding in the next token prediction task. We also designed some case-study examples to show what kind of relations the module is able to capture. Notice that the top 2 predictions of case (2) both form valid tonal sequences (in C major and F major keys, respectively).</p>
	<figure class="figure text-center">
	  <img src="img/case-study.png" class="figure-img img-fluid" style="width: 100%; max-width: 750px;">
	  <figcaption class="figure-caption">Fig. 5. A case study of the module's behavior on different music relations: (1) exact repetition, (2) tonal sequence and (3) modulating sequence. The question mark is the token to predict and the (s) token is the sustain label. The table shows the top two predictions and their probability from the sequential attention model.</figcaption>
	</figure>
	
	Even though the model is not designed for music generation in mind, we tried some music generation experiments with the model. In Fig. 6, we use the conditional self-attentive language model to generate the melody given the chords and partial melody notes. 
	
	<figure class="figure text-center">
	  <img src="img/Generation.png" class="figure-img img-fluid" style="width: 100%; max-width: 800px;">
	  <figcaption class="figure-caption">Fig. 6. A generated sample. All chords and the melody for the first 8 bars are given. The model generates the melody for the next 8 bars. The repetitions in the generated piece are painted in colors (green and red).</figcaption>
	</figure>
	<h4> 4. More Generation Examples </h4>
	
	<p>Below we show another example where the model generates the melody from the beginning given the chords. Notice that the generated melody contains short-term and long-term repetitions, which occurs mainly at the right places (i.e., the melody repeats where the chord sequence repeats). </p>
	
	<p>More generated MIDI samples are available <a href="https://github.com/music-x-lab/SeqAttn/tree/master/midi">here</a>.</p>
	

    <div class="embed-responsive midi-container">
         <iframe class="embed-responsive-item" src="player.html#midi/0_given_64bars_generate_0bars.mid"> </iframe>
    </div>
	<figure class="figure text-center">
    <figcaption class="figure-caption">The original MIDI file from the Nottingham dataset (test set, first 16 bars).</figcaption>
	</figure>

    <div class="embed-responsive midi-container">
         <iframe class="embed-responsive-item" src="player.html#midi/0_given_0bars_generate_64bars.mid"> </iframe>
    </div>
	<figure class="figure text-center">
    <figcaption class="figure-caption">A generated piece. The melody (purple) is generated given the chords (red) using the conditional sequential attention language model.</figcaption>
	</figure>
	
	</div>
	
	<p>Thanks to Google Creative Lab for the <a href="https://github.com/googlecreativelab/chrome-music-lab/tree/master/pianoroll">midi player</a>.
	</p>
	<p>Published and hosted by Github Pages</p>
	<p> </p>
	</div>
	
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
  </body>
</html>